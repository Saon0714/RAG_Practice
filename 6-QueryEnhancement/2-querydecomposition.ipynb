{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76de0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saonpaul/Code/RAG_Practice/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x31a196e40>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x17dde2c00>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some sub-questions that break down the complex query:\n",
      "\n",
      "1. **What memory mechanisms does LangChain utilize?** \n",
      "2. **How do LangChain agents leverage memory?**\n",
      "3. **What memory capabilities does CrewAI offer?**\n",
      "4. **How do CrewAI's agents differ from LangChain agents in terms of memory usage?** \n",
      "\n",
      "\n",
      "These sub-questions target specific aspects of memory and agent functionality in both LangChain and CrewAI, allowing for more focused and precise document retrieval. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: Here are sub-questions to decompose the complex question:\n",
      "A: Please provide the complex question so I can help decompose it into sub-questions. ðŸ˜Š \n",
      "\n",
      "I'm ready to analyze the context you've given and break down the question into smaller, more manageable parts.  \n",
      "\n",
      "\n",
      "\n",
      "Q: **What memory mechanisms does LangChain utilize in its applications?**\n",
      "A: According to the text, LangChain uses **ConversationBufferMemory** and **ConversationSummaryMemory** as memory mechanisms. \n",
      "\n",
      "\n",
      "These modules allow the LLM to:\n",
      "\n",
      "* **Maintain awareness of previous conversation turns** (ConversationBufferMemory)\n",
      "* **Summarize long interactions to fit within token limits** (ConversationSummaryMemory) \n",
      "\n",
      "\n",
      "Q: **How do LangChain agents leverage memory to perform tasks?**\n",
      "A: According to the provided text, LangChain agents use **context-aware memory** across steps.  \n",
      "\n",
      "This means they can remember information from previous interactions in a task and use that context to inform their current decisions and actions. \n",
      "\n",
      "\n",
      "Q: **What memory and agent capabilities are offered by CrewAI?**\n",
      "A: While the provided context describes CrewAI's structure and functionalities, it doesn't explicitly mention specific memory capabilities or agent capabilities beyond:\n",
      "\n",
      "* **Defined Purpose, Goal, and Tools:** Each agent has a clear purpose, goal, and set of tools to accomplish its tasks.\n",
      "* **Structured Workflows:** Agents operate within defined roles (e.g., researcher, planner, executor) and collaborate through structured workflows.\n",
      "* **Context Sharing and Communication:** Agents can share context and dynamically communicate with each other.\n",
      "* **Task Focus:** The framework ensures agents stay on task and contribute to the overall crew objective.\n",
      "\n",
      "\n",
      "To determine the specific memory and agent capabilities offered by CrewAI, you would need to consult additional documentation or resources that delve into the technical details of the framework. \n",
      "\n",
      "\n",
      "Q: **What are the key differences in how LangChain and CrewAI handle memory and agent functionalities?**\n",
      "A: Here's a breakdown of how LangChain and CrewAI handle memory and agent functionalities based on the provided context:\n",
      "\n",
      "**LangChain:**\n",
      "\n",
      "* **Memory:** LangChain agents use **context-aware memory**. This means they can remember information from previous steps in an interaction and use it to make decisions in subsequent steps.\n",
      "* **Agent Functionalities:** LangChain agents operate on a **planner-executor model**.  \n",
      "    * **Planner:**  Analyzes the goal and devises a sequence of tool invocations needed to achieve it. This can involve:\n",
      "        * **Dynamic decision-making:**  Adjusting the plan based on the results of previous tool invocations.\n",
      "        * **Branching logic:**  Taking different paths within the plan depending on conditions.\n",
      "    * **Executor:**  Actually carries out the plan by calling the specified tools.\n",
      "\n",
      "**CrewAI:**\n",
      "\n",
      "* **Memory:** The context doesn't explicitly detail how CrewAI handles memory. It focuses more on its role in **role-based collaboration**.\n",
      "* **Agent Functionalities:** CrewAI's primary focus seems to be on **managing collaboration between different AI \"roles\"**. It likely handles:\n",
      "    * **Assigning tasks:** Distributing responsibilities among the different AI agents based on their specialized roles.\n",
      "    * **Coordinating actions:** Ensuring the different roles work together effectively and in a structured manner.\n",
      "    * **Maintaining context:** Keeping track of the overall progress and shared information across the collaborative process.\n",
      "\n",
      "**Key Differences:**\n",
      "\n",
      "* **Core Focus:** LangChain specializes in **agent planning and execution**, while CrewAI emphasizes **role-based collaboration**.\n",
      "* **Memory Details:** LangChain's memory mechanism is described, while CrewAI's memory management isn't explicitly stated.\n",
      "* **Complementary Nature:** The context suggests that LangChain and CrewAI are designed to work together. LangChain provides the planning and tool execution capabilities, while CrewAI handles the orchestration and collaboration aspects.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions!\n",
      "\n",
      "\n",
      "Q: These sub-questions break down the comparison into specific aspects of memory and agent use in each platform, allowing for more focused and relevant document retrieval\n",
      "A: The provided context highlights the following features related to memory and agent use:\n",
      "\n",
      "**LangChain:**\n",
      "\n",
      "* **Hybrid Retrieval:** Combines keyword-based (BM25) and embedding-based retrieval for better recall, capturing both exact matches and semantically similar content.\n",
      "* **Traceability:**  Provides full traceability of agent decisions and interactions, aiding in debugging and transparency.\n",
      "\n",
      "**CrewAI:**\n",
      "\n",
      "* **Agent Context-Sharing:** Enables agents to pass intermediate data to each other in a structured way, leading to emergent behaviors like delegation, consultation, and review.\n",
      "\n",
      "The sub-questions you mentioned aim to delve deeper into these aspects, comparing how each platform handles:\n",
      "\n",
      "* **Memory Management:** How do they store and access information relevant to agent tasks?\n",
      "* **Agent Collaboration:** How do they facilitate interaction and information exchange between agents?\n",
      "* **Retrieval Strategies:** What methods do they use to locate relevant information for agent responses?\n",
      "\n",
      "\n",
      "By focusing on these specific areas, the sub-questions can provide a more nuanced understanding of the differences and strengths of each platform in managing memory and enabling agent collaboration. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6cb9a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final Response:\n",
      "\n",
      "LangChain and CrewAI are both frameworks for building AI agents, but they differ in their core focus. LangChain specializes in agent planning and execution, utilizing context-aware memory and a planner-executor model. It offers features like hybrid retrieval and traceability. CrewAI, on the other hand, emphasizes role-based collaboration, facilitating communication and context sharing between specialized AI agents.  While CrewAI's specific memory mechanisms are not explicitly detailed, its strength lies in orchestrating complex tasks through collaborative workflows. \n",
      "\n",
      "\n",
      "Essentially, LangChain is geared towards individual agent intelligence and task completion, while CrewAI focuses on coordinating multiple agents towards a shared goal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Given the following intermediate question and answer pairs, provide a concise final answer to the original question.\n",
    "Intermediate Q&A:\n",
    "{intermediate_qa}\"\"\")\n",
    "final_chain = final_prompt | llm | StrOutputParser()\n",
    "final_response = final_chain.invoke({\"intermediate_qa\": final_answer})\n",
    "print(\"\\nâœ… Final Response:\\n\")\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a8a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
